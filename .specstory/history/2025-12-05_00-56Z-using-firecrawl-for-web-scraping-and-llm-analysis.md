<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 744ee03b-df34-4a09-9e9b-d0dd3566db02 (2025-12-05 00:56Z) -->

# Using firecrawl for web scraping and LLM analysis (2025-12-05 00:56Z)

_**User (2025-12-05 00:13Z)**_

read this documentation. then talk to me about the approach of using firecrawl to identify all the pages of the site, then scraping the contents of all the pages, and then using an LLM to identify price information.



Here is a sample.

https://www.upliftedrx.com/ 



# Firecrawl



> Use Firecrawl with Agno to scrape and crawl the web.

**FirecrawlTools** enable an Agent to perform web crawling and scraping tasks.

## Prerequisites

The following example requires the `firecrawl-py` library and an API key which can be obtained from [Firecrawl](https://firecrawl.dev).

```shell  theme={null}

pip install -U firecrawl-py

```

```shell  theme={null}

export FIRECRAWL_API_KEY=***

```

## Example

The following agent will scrape the content from [https://finance.yahoo.com/](https://finance.yahoo.com/) and return a summary of the content:

```python cookbook/tools/firecrawl_tools.py theme={null}

from agno.agent import Agent

from agno.tools.firecrawl import FirecrawlTools

agent = Agent(tools=[FirecrawlTools(enable_scrape=False, enable_crawl=True)], markdown=True)

agent.print_response("Summarize this https://finance.yahoo.com/")

```

## Toolkit Params

| Parameter        | Type             | Default                     | Description                                                                  |

| ---------------- | ---------------- | --------------------------- | ---------------------------------------------------------------------------- |

| `api_key`        | `str`            | `None`                      | API key for authentication. Uses FIRECRAWL\_API\_KEY env var if not provided |

| `enable_scrape`  | `bool`           | `True`                      | Enables website scraping functionality                                       |

| `enable_crawl`   | `bool`           | `False`                     | Enables website crawling functionality                                       |

| `enable_mapping` | `bool`           | `False`                     | Enables website mapping functionality                                        |

| `enable_search`  | `bool`           | `False`                     | Enables web search functionality                                             |

| `all`            | `bool`           | `False`                     | Enables all functionality when set to True                                   |

| `formats`        | `List[str]`      | `None`                      | List of formats to be used for operations                                    |

| `limit`          | `int`            | `10`                        | Maximum number of items to retrieve                                          |

| `poll_interval`  | `int`            | `30`                        | Interval in seconds between polling for results                              |

| `search_params`  | `Dict[str, Any]` | `None`                      | Parameters for search operations                                             |

| `api_url`        | `str`            | `https://api.firecrawl.dev` | API URL to use for the Firecrawl app                                         |

## Toolkit Functions

| Function         | Description                                                                                                                                                                                                                                             |

| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| `scrape_website` | Scrapes a website using Firecrawl. Parameters include `url` to specify the URL to scrape. The function supports optional formats if specified. Returns the results of the scraping in JSON format.                                                      |

| `crawl_website`  | Crawls a website using Firecrawl. Parameters include `url` to specify the URL to crawl, and an optional `limit` to define the maximum number of pages to crawl. The function supports optional formats and returns the crawling results in JSON format. |

| `map_website`    | Maps a website structure using Firecrawl. Parameters include `url` to specify the URL to map. Returns the mapping results in JSON format.                                                                                                               |

| `search`         | Performs a web search using Firecrawl. Parameters include `query` for the search term and optional `limit` for maximum results. Returns search results in JSON format.                                                                                  |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/firecrawl.py)

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/firecrawl_tools.py)

---

> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.agno.com/llms.txt

---

_**Agent (model claude-4.5-sonnet-thinking, mode Ask)**_

<think><details><summary>Thought Process</summary>
The user wants me to:
1. Read the documentation about Firecrawl integration with Agno
2. Discuss the approach of using Firecrawl to identify all pages of a site, then scraping contents, and then using an LLM to extract price information
3. They've provided an example site: https://www.upliftedrx.com/

Since I'm in Ask mode, I should only provide information and guidance, not implement anything.

Let me think about the approach they're asking about:

1. **Mapping phase**: Use Firecrawl's `map_website` function to discover all URLs on the site
2. **Scraping phase**: Use Firecrawl's `scrape_website` or `crawl_website` to get the content from all discovered pages
3. **Extraction phase**: Use an LLM (via Agno Agent) to analyze the scraped content and extract price information

Looking at the documentation:
- `map_website`: Discovers URLs on a website
- `scrape_website`: Scrapes a single URL
- `crawl_website`: Crawls a website (automatically discovers and scrapes multiple pages)
- `search`: Performs web search

The user is looking at the Uplifted RX Medical Aesthetics site which has services and pricing information.

Let me provide a comprehensive analysis of this approach.</details></think>

---

I'll analyze the approach of using Firecrawl with Agno to extract price information from medspa websites like the Uplifted RX example.

## Approach Analysis

### Three-Phase Strategy

**Phase 1: Discovery (Mapping)**
Use `map_website` to identify all pages on the site:
- Maps the entire site structure
- Returns list of all URLs
- Fast discovery without downloading full content

**Phase 2: Content Extraction (Scraping)**
Two options here:

1. **Option A: Use `crawl_website`** (Recommended)
   - Automatically discovers AND scrapes pages
   - More efficient than map + scrape separately
   - Can set `limit` parameter to control scope
   - Returns full content in markdown/HTML format

2. **Option B: Use `map_website` + `scrape_website`**
   - More control over which pages to scrape
   - Can filter discovered URLs before scraping
   - Useful if you want to exclude certain page types

**Phase 3: Information Extraction (LLM Analysis)**
Use Agno Agent to analyze scraped content:
- Agent processes the markdown/HTML content
- Extracts structured price information
- Can identify service names, prices, packages, etc.

### Recommended Implementation Strategy

For your medspa use case, I'd suggest:

```python
# Enable both mapping and crawling
agent = Agent(
    tools=[FirecrawlTools(
        enable_crawl=True,      # For comprehensive scraping
        enable_mapping=True,    # For discovering all pages
        limit=50                # Reasonable limit for medspa sites
    )],
    markdown=True
)

# Approach 1: Crawl directly (simpler)
agent.print_response(
    "Crawl https://www.upliftedrx.com/ and extract all pricing information "
    "including service names, prices, and any package deals"
)

# Approach 2: Map then selectively scrape (more control)
# First discover pages
urls = agent.run("Map all pages on https://www.upliftedrx.com/")
# Then scrape price-related pages
agent.run("Scrape pricing pages and extract all service prices")
```

### Considerations for Uplifted RX Site

Looking at the example site:
- Main services: Aesthetic Injectables, Skin Rejuvenation, Hair Restoration, Wellness & Weight Loss
- Has a dedicated "Price List" page (likely at `/price-list` or similar)
- Service pages may have individual pricing
- Booking system might have pricing info

**Key challenges:**
1. **Pricing might be behind appointment-only systems** - The site shows "By appointment only" which suggests pricing might not be fully public
2. **Dynamic content** - Some pricing might load via JavaScript
3. **Multiple pricing pages** - Prices could be spread across service pages + a price list page

### Optimal Approach for Your Use Case

Given your goal of extracting medspa pricing:

```python
from agno.agent import Agent
from agno.tools.firecrawl import FirecrawlTools

# Step 1: Map the site to find all pages
agent = Agent(
    tools=[FirecrawlTools(enable_mapping=True, enable_scrape=True)],
    markdown=True
)

# Discover all URLs
response = agent.run("Map https://www.upliftedrx.com/")

# Step 2: Crawl with focus on pricing pages
agent_crawler = Agent(
    tools=[FirecrawlTools(enable_crawl=True, limit=30)],
    markdown=True
)

# Step 3: Extract structured pricing
agent_crawler.print_response(
    "Crawl https://www.upliftedrx.com/ and extract all pricing information. "
    "For each service, provide: service name, price (or price range), "
    "category, and any special offers or packages. Format as structured data."
)
```

### Alternative: Use Firecrawl's Extract Feature

Firecrawl also has an `extract` function (via MCP) that can combine scraping + LLM extraction in one step with a defined schema. This would be more efficient than scrape-then-analyze.

### Pros and Cons

**Pros:**
- Automated discovery of all pages
- Handles JavaScript rendering
- Clean markdown output for LLM processing
- Rate limiting handled by Firecrawl

**Cons:**
- API costs (Firecrawl charges per page)
- Pricing might not be public on all medspa sites
- May need multiple crawls if pricing is behind forms
- Rate limits on free tier

Would you like me to explain how to integrate this into your existing `medspa_extraction.py` tool, or discuss how to structure the LLM prompts for optimal price extraction?

---

_**User (2025-12-05 00:13Z)**_

Build out this tool now

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
The user wants me to build out a Firecrawl-based tool for extracting medspa pricing information. They've asked me to implement this now, and I'm in Agent mode so I can make changes.

Let me first read the existing files to understand the current structure:
1. tools/medspa_extraction.py - to see what's already there
2. requirements.txt - to check dependencies
3. glowscape_agent.py - to understand how tools are integrated

Then I'll:
1. Add firecrawl-py to requirements.txt
2. Create or update the medspa extraction tool to use Firecrawl
3. Make sure it integrates with the existing Agno agent structure</details></think>

---

I'll build out a Firecrawl-based tool for medspa pricing extraction. Let me first check the existing code structure.

---

<tool-use data-tool-type="unknown" data-tool-name="read_file_v2">
<details>
<summary>Tool use: **read_file_v2**</summary>


Parameters:

```json
{
  "targetFile": "c:\\CodeWork\\overclock\\GlowScape\\tools\\medspa_extraction.py",
  "charsLimit": 1000000,
  "effectiveUri": "c:\\CodeWork\\overclock\\GlowScape\\tools\\medspa_extraction.py"
}
```
Result:

```json
{
  "contents": "\"\"\"\r\nMedSpa Extraction Tool\r\n\r\nTool for extracting MedSpa business data (uses mock data for testing).\r\n\"\"\"\r\n\r\nimport json\r\nimport random\r\nfrom agno.tools import tool\r\n\r\n\r\n# =============================================================================\r\n# Mock Data Configuration\r\n# =============================================================================\r\n\r\nMOCK_MEDSPA_PREFIXES = [\r\n    \"Glow\", \"Radiance\", \"Serenity\", \"Luxe\", \"Pure\", \"Bella\", \"Zen\", \r\n    \"Revive\", \"Oasis\", \"Elite\", \"Harmony\", \"Vitality\", \"Azure\", \"Bloom\"\r\n]\r\n\r\nMOCK_MEDSPA_SUFFIXES = [\r\n    \"MedSpa\", \"Medical Spa\", \"Aesthetics\", \"Wellness Center\", \r\n    \"Beauty Clinic\", \"Skin Studio\", \"Rejuvenation Center\"\r\n]\r\n\r\nMOCK_SERVICES = [\r\n    \"Botox\", \"Fillers\", \"Laser Hair Removal\", \"Chemical Peels\", \r\n    \"Microneedling\", \"HydraFacial\", \"Body Contouring\", \"IV Therapy\",\r\n    \"PRP Treatments\", \"Skin Tightening\", \"Lip Augmentation\", \"Kybella\"\r\n]\r\n\r\nMOCK_STREETS = [\r\n    \"Main St\", \"Oak Ave\", \"Maple Dr\", \"Cedar Blvd\", \"Pine St\",\r\n    \"Willow Way\", \"Park Ave\", \"Broadway\", \"Center St\", \"Lake Dr\"\r\n]\r\n\r\n\r\n# =============================================================================\r\n# Helper Functions\r\n# =============================================================================\r\n\r\ndef _generate_mock_medspa(city: str, index: int) -&gt; dict:\r\n    \"\"\"Generate a single mock MedSpa entry.\r\n    \r\n    Args:\r\n        city: The city name for the address\r\n        index: Index number for the MedSpa\r\n        \r\n    Returns:\r\n        Dictionary containing mock MedSpa business information\r\n    \"\"\"\r\n    prefix = random.choice(MOCK_MEDSPA_PREFIXES)\r\n    suffix = random.choice(MOCK_MEDSPA_SUFFIXES)\r\n    street_num = random.randint(100, 9999)\r\n    street = random.choice(MOCK_STREETS)\r\n    \r\n    return {\r\n        \"id\": f\"medspa_{index}_{random.randint(1000, 9999)}\",\r\n        \"name\": f\"{prefix} {suffix}\",\r\n        \"address\": f\"{street_num} {street}, {city}\",\r\n        \"phone\": f\"({random.randint(200, 999)}) {random.randint(100, 999)}-{random.randint(1000, 9999)}\",\r\n        \"website\": f\"https://www.{prefix.lower()}{suffix.lower().replace(' ', '')}.com\",\r\n        \"rating\": round(random.uniform(3.5, 5.0), 1),\r\n        \"review_count\": random.randint(10, 500),\r\n        \"services\": random.sample(MOCK_SERVICES, k=random.randint(3, 6)),\r\n        \"hours\": \"Mon-Fri: 9AM-6PM, Sat: 10AM-4PM\",\r\n    }\r\n\r\n\r\n# =============================================================================\r\n# MedSpa Extraction Tool\r\n# =============================================================================\r\n\r\n@tool\r\ndef medspa_extraction_tool(city: str) -&gt; str:\r\n    \"\"\"\r\n    Extract MedSpa locations from a specified city using Google Maps/Places data.\r\n    \r\n    Use this tool when the user wants to find, extract, or list MedSpa businesses\r\n    in a particular city or geographic area.\r\n    \r\n    Args:\r\n        city: The city or geographic area to search for MedSpa locations.\r\n        \r\n    Returns:\r\n        JSON string containing a list of MedSpa businesses with their details.\r\n    \"\"\"\r\n    # Generate 10 mock MedSpa entries\r\n    medspas = [_generate_mock_medspa(city, i) for i in range(1, 11)]\r\n    \r\n    result = {\r\n        \"city\": city,\r\n        \"total_results\": len(medspas),\r\n        \"source\": \"Google Maps/Places via Apify (MOCK DATA)\",\r\n        \"medspas\": medspas\r\n    }\r\n    \r\n    return json.dumps(result, indent=2)\r\n",
  "totalLinesInFile": 99
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="read_file_v2">
<details>
<summary>Tool use: **read_file_v2**</summary>


Parameters:

```json
{
  "targetFile": "c:\\CodeWork\\overclock\\GlowScape\\requirements.txt",
  "charsLimit": 1000000,
  "effectiveUri": "c:\\CodeWork\\overclock\\GlowScape\\requirements.txt"
}
```
Result:

```json
{
  "contents": "agno&gt;=1.0.0\r\nopenai&gt;=1.0.0\r\npython-dotenv&gt;=1.0.0\r\napify-client&gt;=1.0.0\r\nfastapi&gt;=0.104.0\r\nuvicorn[standard]&gt;=0.24.0\r\npydantic&gt;=2.0.0\r\n\r\n",
  "totalLinesInFile": 9
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="read_file_v2">
<details>
<summary>Tool use: **read_file_v2**</summary>


Parameters:

```json
{
  "targetFile": "c:\\CodeWork\\overclock\\GlowScape\\glowscape_agent.py",
  "charsLimit": 1000000,
  "effectiveUri": "c:\\CodeWork\\overclock\\GlowScape\\glowscape_agent.py"
}
```
Result:

```json
{
  "contents": "\"\"\"\r\nGlowScape Agent - MedSpa Location Data Extraction Agent\r\n\r\nA conversational AI agent specialized in extracting and analyzing \r\nMedSpa business information from Google Maps/Places.\r\n\"\"\"\r\n\r\nimport os\r\nfrom textwrap import dedent\r\n\r\nfrom dotenv import load_dotenv\r\nfrom agno.agent import Agent\r\nfrom agno.models.openai import OpenAIChat\r\n\r\nfrom tools import google_maps_search, yelp_scraper\r\n\r\nload_dotenv(\".env.local\")\r\n\r\nGLOWSCAPE_DESCRIPTION = dedent(\"\"\"\r\n    You are GlowScape, a specialized AI agent designed to help users discover and extract \r\n    comprehensive data about MedSpa (Medical Spa) businesses. You have expertise in:\r\n    \r\n    - Locating MedSpa businesses across cities and regions\r\n    - Extracting business details including names, addresses, phone numbers, and websites\r\n    - Gathering ratings, reviews, and operational hours\r\n    - Identifying service offerings and specializations\r\n    - Compiling structured datasets for analysis\r\n    \r\n    You have access to multiple data sources including Google Maps and Yelp, and can use\r\n    the appropriate tool based on the user's specific request.\r\n\"\"\")\r\n\r\nGLOWSCAPE_INSTRUCTIONS = [\r\n    \"Introduce yourself as GlowScape when users first interact with you.\",\r\n    \"Explain your capabilities clearly when asked what you can do.\",\r\n    \"You have access to both Google Maps and Yelp scraping tools.\",\r\n    \"Use the google_maps_search tool ONLY when the user explicitly asks to use Google Maps or Google Places.\",\r\n    \"Use the yelp_scraper tool ONLY when the user explicitly asks to use Yelp.\",\r\n    \"If the user doesn't specify a data source, ask them which platform they'd like to use (Google Maps or Yelp).\",\r\n    \"After using a tool, clearly state which data source was used.\",\r\n    \"Provide structured, organized responses when presenting business data.\",\r\n    \"Be helpful in explaining the data fields you can extract (name, address, phone, website, ratings, reviews, hours, etc.).\",\r\n]\r\n\r\n\r\ndef create_glowscape_agent() -&gt; Agent:\r\n    \"\"\"Create and configure the GlowScape agent.\"\"\"\r\n    # Validate Apify API token for Google Maps tool\r\n    apify_token = os.getenv(\"APIFY_API_TOKEN\")\r\n    if not apify_token:\r\n        raise ValueError(\r\n            \"APIFY_API_TOKEN not found in environment. \"\r\n            \"Please add it to your .env.local file.\"\r\n        )\r\n    \r\n    agent = Agent(\r\n        name=\"GlowScape\",\r\n        model=OpenAIChat(id=\"gpt-4o-mini\"),\r\n        description=GLOWSCAPE_DESCRIPTION,\r\n        instructions=GLOWSCAPE_INSTRUCTIONS,\r\n        tools=[google_maps_search, yelp_scraper],\r\n        markdown=True,\r\n        add_datetime_to_context=True,\r\n    )\r\n    return agent\r\n\r\n\r\ndef main():\r\n    \"\"\"Run the GlowScape agent in interactive terminal mode.\"\"\"\r\n    agent = create_glowscape_agent()\r\n    \r\n    print(\"\\n\" + \"=\" * 60)\r\n    print(\"  GlowScape - MedSpa Location Data Agent\")\r\n    print(\"=\" * 60)\r\n    print(\"Type 'quit' or 'exit' to end the session.\\n\")\r\n    \r\n    # Initial greeting\r\n    agent.print_response(\"Introduce yourself and explain what you can help with.\", stream=True)\r\n    \r\n    # Interactive loop\r\n    while True:\r\n        try:\r\n            user_input = input(\"\\n\\nYou: \").strip()\r\n            \r\n            if not user_input:\r\n                continue\r\n                \r\n            if user_input.lower() in (\"quit\", \"exit\", \"q\"):\r\n                print(\"\\nGoodbye! Thank you for using GlowScape.\")\r\n                break\r\n            \r\n            print(\"\\nGlowScape: \", end=\"\")\r\n            agent.print_response(user_input, stream=True)\r\n            \r\n        except KeyboardInterrupt:\r\n            print(\"\\n\\nSession interrupted. Goodbye!\")\r\n            break\r\n        except Exception as e:\r\n            print(f\"\\nError: {e}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n",
  "totalLinesInFile": 105
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Now I understand the structure. I need to:
1. Add firecrawl-py to requirements.txt
2. Create a new Firecrawl-based pricing extraction tool in the tools directory
3. The tool should:
   - Take a website URL as input
   - Use Firecrawl to map and crawl the site
   - Extract pricing information using structured prompts
   - Return structured pricing data

Let me create a comprehensive tool that uses Firecrawl's capabilities.</details></think>

---

## Plan

I'll create a comprehensive Firecrawl-based tool for extracting medspa pricing information:

1. **Add firecrawl-py dependency** to requirements.txt
2. **Create new pricing extraction tool** that:
   - Maps website structure to find all pages
   - Crawls relevant pages (especially pricing pages)
   - Extracts structured pricing data using LLM
3. **Integrate with existing agent** structure

Let me build this now:

---

<tool-use data-tool-type="unknown" data-tool-name="edit_file_v2">
<details>
<summary>Tool use: **edit_file_v2**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "c:\\CodeWork\\overclock\\GlowScape\\requirements.txt"
}
```
Additional data:

```json
{
  "codeblockId": "0bf3e75b-04db-45b4-a911-65742565ffa0"
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="edit_file_v2">
<details>
<summary>Tool use: **edit_file_v2**</summary>


Parameters:

```json
{
  "relativeWorkspacePath": "c:\\CodeWork\\overclock\\GlowScape\\tools\\pricing_extraction.py"
}
```
Additional data:

```json
{
  "codeblockId": "fb354774-58b8-4e19-85b4-ceff4dfecbdd"
}
```
Status: **loading**


</details>
</tool-use>

---



---

